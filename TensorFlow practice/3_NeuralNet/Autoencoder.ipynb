{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 784\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_steps = 20000\n",
    "batch_size = 256\n",
    "disply_step = 1000\n",
    "\n",
    "num_hidden_1 = 128\n",
    "num_hidden_2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train.astype(np.float32), x_test.astype(np.float32)\n",
    "\n",
    "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "\n",
    "x_train, x_test = x_train/255., x_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(10000).batch(batch_size).prefetch(1)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_data = test_data.repeat().batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_normal = tf.initializers.RandomNormal()\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(random_normal([num_features, num_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(random_normal([num_hidden_1, num_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(random_normal([num_hidden_2, num_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(random_normal([num_hidden_1, nums_features])),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(random_normal([num_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(random_normal([num_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(random_normal([num_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(random_normal([num_features])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([784, 128])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['encoder_h1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                biases['encoder_b1']))\n",
    "    \n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                biases['encoder_b2']))\n",
    "    \n",
    "    return layer_2\n",
    "\n",
    "def decoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                  biases['decoder_b1']))\n",
    "    \n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                  biases['decoder_b2']))\n",
    "    \n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square(reconstructed, original):\n",
    "    return tf.reduce_mean(tf.pow(original -reconstructed, 2))\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(x):\n",
    "    with tf.GradientTape() as g:\n",
    "        reconstructed_image = decoder(encoder(x))\n",
    "        loss = mean_square(reconstructed_image, x)\n",
    "        \n",
    "    trainable_variables = list(set(weights.values()) | set(biases.values()))\n",
    "\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.236298\n",
      "step: 1000, loss: 0.017984\n",
      "step: 2000, loss: 0.011823\n",
      "step: 3000, loss: 0.008842\n",
      "step: 4000, loss: 0.008418\n",
      "step: 5000, loss: 0.007441\n",
      "step: 6000, loss: 0.006401\n",
      "step: 7000, loss: 0.006229\n",
      "step: 8000, loss: 0.005942\n",
      "step: 9000, loss: 0.005629\n",
      "step: 10000, loss: 0.005322\n",
      "step: 11000, loss: 0.004981\n",
      "step: 12000, loss: 0.005280\n",
      "step: 13000, loss: 0.004751\n",
      "step: 14000, loss: 0.004349\n",
      "step: 15000, loss: 0.004742\n",
      "step: 16000, loss: 0.004616\n",
      "step: 17000, loss: 0.004141\n",
      "step: 18000, loss: 0.004663\n",
      "step: 19000, loss: 0.004185\n",
      "step: 20000, loss: 0.003846\n"
     ]
    }
   ],
   "source": [
    "for step, (batch_x, _) in enumerate(train_data.take(training_steps+1)):\n",
    "    \n",
    "    loss = run_optimization(batch_x)\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        print(\"step: %i, loss: %f\" % (step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 700 into shape (28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-5a04667e8abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mreconstr_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstructed_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mcanvas_recon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstr_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 700 into shape (28,28)"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "canvas_orig = np.empty((28*n, 28*n))\n",
    "canvas_recon = np.empty((28*n, 28*n))\n",
    "\n",
    "for i, (batch_x, _) in enumerate(test_data.take(n)):\n",
    "    reconstructed_images = decoder(encoder(batch_x))\n",
    "    for j in range(n):\n",
    "        img = batch_x[j].numpy().reshape([28,28])\n",
    "        canvas_orig[i*28:(i+1)*28, j*28:(j+1)*28] = img\n",
    "        \n",
    "        for j in range(n):\n",
    "            reconstr_img = reconstructed_images[j].numpy().reshape([28,28])\n",
    "            canvas_recon[i*28:(i+1)*28, j*28:(j+1)*28] = reconstr_img \n",
    "            \n",
    "print('original images')\n",
    "plt.figure(figsize=(n,n))\n",
    "plt.imshow(canvas_orig, origin=\"uppder\", cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('reconstructed images')\n",
    "plt.figure(figsize=(n,n))\n",
    "plt.imshow(canvas_recon, origin=\"uppder\", cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
